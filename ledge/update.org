#+TITLE: Update

#+PROPERTY: header-args :eval never :tangle ./update.py

#+BEGIN_SRC python :exports none
  """
  This file is generated using an accompanying org file.
  Do not edit manually.
  """
#+END_SRC

This module contains implementations of some weight update techniques. Weight
update here is decoupled from the handling of lag and take the history of losses
instead of just one step since we might need to refit weights at each time step
(though the history can be of length 1).

Each (implemented) function in this module takes at least two args and return a
weight xarray:

1. List of ~Loss~ (as defined in [[./datatypes.org]])
2. Keyword argument ~init_weights~ for starting weights

#+BEGIN_SRC python :exports none
  import xarray as xr
  import numpy as np
  from typing import List, Callable
  from ledge.datatypes import Loss, Weight
  from ledge.utils import uniform_weights
  from inspect import signature
  from functools import reduce
  import operator
#+END_SRC

* Noop
A do nothing function. This might be useful to maintain consistency while
building models.

#+BEGIN_SRC python
  def noop(losses: List[Loss], init_weights=None) -> Weight:
      """
      Do nothing.
      """

      models = [loss.attrs["model"] for loss in losses]
      if init_weights is None:
          return uniform_weights(models, ones=False)
      else:
          return init_weights
#+END_SRC

* Mixer
Merge function for mixing update functions using specified weights.

#+BEGIN_SRC python
  def create_mixer(update_fns: List[Callable], mixing_weights: np.ndarray) -> Callable:
      """
      Use weights (assumed to be normalized) to mix the update functions
      """

      def _mix_update(losses: List[Loss], init_weights=None) -> Weight:

          weights = []

          for update_fn, mw in zip(update_fns, mixing_weights):
              # Don't overwrite the init_weights if present in the component updater
              iw = signature(update_fn).parameters["init_weights"].default or init_weights
              weights.append(update_fn(losses, init_weights=iw) * mw)

          return reduce(operator.add, weights[1:], weights[0])

      return _mix_update
#+END_SRC

* Pick one
Function that always picks one model and assigns full weight to it.

#+BEGIN_SRC python
  def pick(losses: List[Loss], index: int, init_weights=None) -> Weight:
      """
      Return 1 weight for model at index and 0 for others
      """

      models = [loss.attrs["model"] for loss in losses]

      weight_vec = np.zeros(len(losses))
      weight_vec[index] = 1
      return xr.DataArray(weight_vec, dims="model", coords={ "model": models })
#+END_SRC

* Follow the leader
This is a simple update which assigns full weight to the model which minimizes
the cumulative loss till the given step. It does good when there is a clear
winner which stays the best without switching a lot.

Optionally, providing a value for /k/ says to follow /k/ leaders (by assigning equal
weights to all of them) instead of just one. /lookback/ tells how many timesteps
to look back.

#+BEGIN_SRC python
  def ftl(losses: List[Loss], k=1, lookback=None, init_weights=None) -> Weight:
      """
      Follow the leader update. Give full weight to the k models with least loss.
      lookback says how many steps to sum the losses for.
      """

      if lookback is None or lookback < 0:
          lookback = 0

      loss_sums = [np.sum(loss[-lookback:]) for loss in losses]
      best_indices = np.argsort(loss_sums)[:k]

      weight_vec = np.zeros(len(losses))
      weight_vec[best_indices] = 1 / k

      models = [loss.attrs["model"] for loss in losses]
      return xr.DataArray(weight_vec, dims="model", coords={ "model" : models })
#+END_SRC

* TODO Follow the perturbed leader
Similar to follow the leader but with added perturbations before deciding who
the leader is. Need to look in the effect to lag here.

#+BEGIN_SRC python
  def ftpl(losses: List[Loss]) -> Weight:
      """
      Follow the perturbed leader update.
      """

      raise NotImplementedError()
#+END_SRC

* Fixed share
Fixes share update as described in Herbster, M., & Warmuth, M. K., Tracking the
best expert, Machine learning, 32(2), 151–178 (1998). This provides regret
gaurantees with respect to the best changing sequence of experts.

#+BEGIN_SRC python
  def fixed_share(losses: List[Loss], eta: float, alpha: float, init_weights=None) -> Weight:
      r"""
      Fixed share update.
      """

      models = [loss.attrs["model"] for loss in losses]
      M = len(models)
      T = len(losses[0])

      if init_weights is None:
          weights = uniform_weights(models, ones=False)
      else:
          weights = init_weights

      # Vectorize this
      for t in range(T):
          vs = weights * np.exp([-eta * loss[t] for loss in losses])
          weights = (alpha * np.sum(vs) / M) + (1 - alpha) * vs

      return weights
#+END_SRC

* TODO Variable share
Variable share update as described in Herbster, M., & Warmuth, M. K., Tracking
the best expert, Machine learning, 32(2), 151–178 (1998).

#+BEGIN_SRC python
  def variable_share(losses: List[Loss]) -> Weight:
      r"""
      Variable share update.
      """

      raise NotImplementedError()
#+END_SRC

* Multiplicative weights
Vanilla multiplicative weight algorithm from Arora, S., Hazan, E., & Kale, S.,
The multiplicative weights update method: a meta-algorithm and applications.,
Theory of Computing, 8(1), 121–164 (2012).

#+BEGIN_SRC python
  def mw(losses: List[Loss], eta: float, init_weights=None) -> Weight:
      r"""
      Multiplicative weight update. :math:`w_i(t + 1) = w_i(t) (1 - \eta m_i(t))`
      """

      models = [loss.attrs["model"] for loss in losses]

      if init_weights is None:
          init_weights = uniform_weights(models)

      updates = [np.prod(1 - eta * loss) for loss in losses]

      return init_weights * updates
#+END_SRC

* Hedging
Hedging from Freund, Y., & Schapire, R. E., A decision-theoretic generalization
of on-line learning and an application to boosting, Journal of computer and
system sciences, 55(1), 119–139 (1997).

#+BEGIN_SRC python
  def hedge(losses: List[Loss], eta: float, init_weights=None) -> Weight:
      r"""
      Exponential weight update. :math:`w_i(t + 1) = w_i(t) e^{- \eta m_i(t)}`
      """

      models = [loss.attrs["model"] for loss in losses]

      if init_weights is None:
          init_weights = uniform_weights(models)

      updates = [np.exp(-eta * np.sum(loss)) for loss in losses]

      return init_weights * updates
#+END_SRC
